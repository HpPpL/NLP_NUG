{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Основаная часть кода:"
      ],
      "metadata": {
        "id": "VhU41sOw14Qt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuNuRtCYnvgm"
      },
      "outputs": [],
      "source": [
        "# Импорт необходимых библиотек.\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка библиотеки для обработки текстов.\n",
        "!pip install deeppavlov"
      ],
      "metadata": {
        "id": "RARw_3CAnyzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Скачивание надстройки.\n",
        "!python -m deeppavlov install morpho_ru_syntagrus_pymorphy"
      ],
      "metadata": {
        "id": "ZzXo0etdoiCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание папки с результатами.\n",
        "os.mkdir(\"Results\")\n",
        "os.chdir(\"Results\")\n",
        "os.mkdir(\"Metadata\")\n",
        "os.mkdir(\"Morphologized words\")\n",
        "os.mkdir(\"Tokenized sentences\")\n",
        "os.chdir(\"..\")"
      ],
      "metadata": {
        "id": "MOf8VL-lE8Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание папки для хранения текстовых данных.\n",
        "os.mkdir(\"Text_Data\")\n",
        "os.chdir(\"Text_Data\")"
      ],
      "metadata": {
        "id": "S668lDWUpc6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Основная часть алгоритма.\n",
        "\n",
        "# Создаем list со всеми именами файлов в папке \"Text_Data\"\n",
        "LoF = os.listdir()\n",
        "\n",
        "# Создаем модель для лемматизации слов.\n",
        "from deeppavlov import build_model, configs\n",
        "model_tr = build_model(configs.morpho_tagger.UD2_0.morpho_ru_syntagrus_pymorphy_lemmatize, download=True)\n",
        "\n",
        "# Создаем DataFrame, в котором будут храниться meta-данные.\n",
        "df_meta = pd.DataFrame(index=range(len(LoF)), columns=[\"Text id\", \"Number of tokens\", \"Number of lemmas\", \"Number of sentences\", \"TTR\", \"Discipline\"])\n",
        "\n",
        "# Начинаем обработку файлов в цикле.\n",
        "for FileIndex in range(len(LoF)):\n",
        "\n",
        "  # Создаем необходимые переменные.\n",
        "  dict_tkn = dict()\n",
        "  dict_lem = dict()\n",
        "  tknz_sent = list()\n",
        "  st_tr = str()\n",
        "\n",
        "  # Выгружаем текстовый файл в list.\n",
        "  fin = open(LoF[FileIndex], 'r', encoding='utf8')\n",
        "  txt = fin.readlines()\n",
        "  fin.close()\n",
        "\n",
        "  # Предобрабатываем текст.\n",
        "  for i in range(len(txt)):\n",
        "    txt[i] = txt[i].replace(\"\\ufeff\", \"\")\n",
        "    txt[i] = txt[i].replace(\"\\n\", \"\")\n",
        "    txt[i] = txt[i].replace(\"\\t\", \"\")\n",
        "\n",
        "  # Создем список предложений.\n",
        "  tknz_sent = ''.join(txt)\n",
        "  tknz_sent = sent_tokenize(tknz_sent)\n",
        "\n",
        "  # Обрабатываем текстовые данные и сохраняем результат в строку.\n",
        "  for parse in model_tr(txt):\n",
        "    st_tr += parse\n",
        "\n",
        "  # Начинаем обработку полученной строки, форматируя данные и очищая спец. символы.\n",
        "  lst_tr = st_tr.split('\\n')\n",
        "  rs_tr = list()\n",
        "\n",
        "  for part in lst_tr:\n",
        "    part_fragmented = part.split('\\t')\n",
        "\n",
        "    # Проверка нужна только для конца, потом что-нибудь придумаю.\n",
        "    if len(part_fragmented) >= 2:\n",
        "      rs_tr += [part_fragmented[0:4] + part_fragmented[5:6]]\n",
        "  \n",
        "  # Выгружаем слова и леммы в словарь.\n",
        "  for i in range(len(rs_tr)):\n",
        "\n",
        "    # Загрузка слова.\n",
        "    if rs_tr[i][1] in dict_tkn:\n",
        "      dict_tkn[rs_tr[i][1]] += 1\n",
        "    else:\n",
        "      dict_tkn[rs_tr[i][1]] = 1\n",
        "\n",
        "    # Загрузка леммы.\n",
        "    if rs_tr[i][2] in dict_lem:\n",
        "      dict_lem[rs_tr[i][2]] += 1\n",
        "    else:\n",
        "      dict_lem[rs_tr[i][2]] = 1\n",
        "\n",
        "  # Добавляем строку данных в meta-данные.\n",
        "  TTR = len(dict_lem) / len(dict_tkn)\n",
        "  df_meta.iloc[FileIndex] = [FileIndex + 1, len(dict_tkn), len(dict_lem), len(tknz_sent), TTR, \"Sociologia\"]\n",
        "\n",
        "  # Начинаем выгружать результаты.\n",
        "\n",
        "  # В папке \"Morphologized words\" находятся xlsx-файл с разобранными словами из текстового файла.\n",
        "  df_mrph = pd.DataFrame(rs_tr, columns=[\"Index\", \"Word\", \"Lemma\", \"Part of speech\", \"Facts\"])\n",
        "\n",
        "  os.chdir(\"../Results/Morphologized words\")\n",
        "  df_mrph.to_excel(LoF[FileIndex][0 : len(LoF[FileIndex]) - 4] + \"_morph\" + \".xlsx\")\n",
        "  os.chdir(\"..\")\n",
        "  os.chdir(\"../Text_Data\")\n",
        "  \n",
        "  # Папка \"Tokenized sentences\" cодержит в себе текст, который был разбит на предложения.\n",
        "  df_snt = pd.DataFrame(tknz_sent, columns=[\"Sentences\"])\n",
        "\n",
        "  os.chdir(\"../Results/Tokenized sentences\")\n",
        "  df_snt.to_excel(LoF[FileIndex][0 : len(LoF[FileIndex]) - 4] + \"_sentences\" + \".xlsx\")\n",
        "  os.chdir(\"..\")\n",
        "  os.chdir(\"../Text_Data\")\n",
        "\n",
        "# В папке \"Metadata\" будут количественные данные об анализируемом тексте.\n",
        "# Выгружаем результаты из df_meta после работы алгоритма.\n",
        "os.chdir(\"../Results/Metadata\")\n",
        "df_meta.to_excel(\"Sociologia\" + \"_metadata\" + \".xlsx\")\n",
        "os.chdir(\"..\")\n",
        "os.chdir(\"../Text_Data\")"
      ],
      "metadata": {
        "id": "UMBRYnzECyGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Скачивание результатов.\n",
        "os.chdir(\"..\")\n",
        "!zip -r ./Results.zip '/content/Results'\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/Results.zip\")\n",
        "\n",
        "os.chdir(\"Text_Data\")"
      ],
      "metadata": {
        "id": "3h_77dWTrvqv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}